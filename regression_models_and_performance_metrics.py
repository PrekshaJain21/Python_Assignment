# -*- coding: utf-8 -*-
"""Regression Models and Performance Metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15vUr2o7E81c5P6Hjfjb7N_Dw_2m8CUEV

# **Supervised Learning: Regression**
#**Models and Performance Metrics**
# ***Total Marks: 200***

Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose

**Answer1:**

**Simple Linear Regression (SLR)** is a **statistical method** used to model the relationship between **two variables** â€” one **independent variable (X)** and one **dependent variable (Y)** â€” by fitting a **straight line** to the data.

The **formula** for SLR is:
[
Y = b_0 + b_1X + \varepsilon
]
where:

* ( Y ) = Dependent variable (the one we want to predict)
* ( X ) = Independent variable (the predictor)
* ( b_0 ) = Intercept (value of Y when X = 0)
* ( b_1 ) = Slope (change in Y for each unit change in X)
* ( \varepsilon ) = Error term (difference between actual and predicted values)

---

### ðŸŽ¯ **Purpose of Simple Linear Regression**

1. **Prediction:**
   To predict the value of one variable based on another (e.g., predicting sales based on advertising spend).

2. **Relationship Analysis:**
   To understand how the dependent variable changes when the independent variable changes.

3. **Trend Estimation:**
   To identify trends or patterns in data (e.g., how temperature affects ice cream sales).

---

### **Example**

Suppose you want to predict a studentâ€™s exam score based on the number of study hours:
[
\text{Score} = b_0 + b_1(\text{Study Hours})
]
If the regression line is ( \text{Score} = 40 + 5(\text{Study Hours}) ),
then each additional study hour increases the predicted score by 5 marks.

---

Question 2: What are the key assumptions of Simple Linear Regression?

**Answer:**

Simple Linear Regression (SLR) relies on several key **assumptions** to ensure that the modelâ€™s results are valid and reliable.

Here are the **main assumptions of SLR** ðŸ‘‡

---

### **1. Linearity**

* The relationship between the **independent variable (X)** and the **dependent variable (Y)** should be **linear**.
* This means changes in X produce proportional changes in Y.
  âœ… *Example:* If study hours increase, exam score should increase (or decrease) in a straight-line pattern.

---

### **2. Independence of Errors**

* The residuals (errors) should be **independent** of each other.
* No correlation should exist between consecutive error terms.
  âœ… *Example:* In time-series data, one prediction error shouldnâ€™t depend on the previous one.

---

### **3. Homoscedasticity (Constant Variance of Errors)**

* The variance of residuals should be **constant** across all values of X.
* If variance increases or decreases (a â€œfan shapeâ€), itâ€™s called **heteroscedasticity**, which violates the assumption.

---

### **4. Normality of Errors**

* The residuals (errors) should be **normally distributed** (bell-shaped).
* This helps in making valid statistical inferences (like hypothesis tests or confidence intervals).

---

### **5. No Multicollinearity**

* Since SLR has only **one independent variable**, this assumption mainly applies to multiple regression.
* However, in SLR, we assume the independent variable is **not constant** and provides **unique information**.

---

### **6. No Significant Outliers**

* Outliers can **distort** the regression line and affect the slope and intercept.
* Itâ€™s important to check for and handle extreme values before modeling.

---

### ðŸ§  **Summary Table**

| Assumption       | Meaning                                | Violation Effect          |
| ---------------- | -------------------------------------- | ------------------------- |
| Linearity        | Relationship between X and Y is linear | Wrong model form          |
| Independence     | Errors are independent                 | Biased estimates          |
| Homoscedasticity | Equal variance of errors               | Inefficient predictions   |
| Normality        | Errors are normally distributed        | Invalid statistical tests |
| No Outliers      | No extreme data points                 | Skewed regression line    |

---

Question 3: Write the mathematical equation for a simple linear regression model and explain each term.

**Answer:**

The **mathematical equation** for a **Simple Linear Regression (SLR)** model is:

[
Y = b_0 + b_1X + \varepsilon
]

---

### ðŸ”¹ **Explanation of Each Term**

| Term            | Meaning                                       | Description                                                                                                                        |
| --------------- | --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| ( Y )           | **Dependent Variable (Response Variable)**    | The variable we are trying to predict or explain.                                                                                  |
| ( X )           | **Independent Variable (Predictor Variable)** | The variable used to predict the value of ( Y ).                                                                                   |
| ( b_0 )         | **Intercept (Constant Term)**                 | The value of ( Y ) when ( X = 0 ). It represents the starting point of the regression line on the Y-axis.                          |
| ( b_1 )         | **Slope Coefficient**                         | It represents the **change in ( Y )** for a **one-unit change in ( X )**. It shows the strength and direction of the relationship. |
| ( \varepsilon ) | **Error Term (Residual)**                     | The difference between the actual value and the predicted value of ( Y ). It accounts for randomness or unobserved factors.        |

---

### ðŸ“˜ **Example**

If we are predicting a studentâ€™s **Exam Score (Y)** based on **Study Hours (X)**:

[
\text{Exam Score} = 40 + 5(\text{Study Hours}) + \varepsilon
]

Interpretation:

* ( b_0 = 40 ): If a student studies 0 hours, the expected score is 40.
* ( b_1 = 5 ): For every additional hour of study, the exam score increases by 5 marks.
* ( \varepsilon ): Random variation due to other factors like sleep, health, or luck.

---

Question 4: Provide a real-world example where simple linear regression can be
applied.

**Answer:**

A **real-world example** of applying **Simple Linear Regression (SLR)** is predicting **house prices** based on **size (area in square feet)**.

---

### ðŸ  **Example: Predicting House Prices**

#### **Scenario:**

A real estate company wants to predict the **price of a house (Y)** using its **size in square feet (X)**.

#### **Data Example:**

| House Size (sq ft) | Price (â‚¹ in lakhs) |
| ------------------ | ------------------ |
| 1000               | 50                 |
| 1500               | 65                 |
| 2000               | 80                 |
| 2500               | 95                 |
| 3000               | 110                |

---

### **Regression Equation:**

[
\text{Price} = b_0 + b_1 (\text{Size}) + \varepsilon
]

After applying regression, suppose we get:

[
\text{Price} = 20 + 0.03 \times (\text{Size})
]

---

### **Interpretation:**

* ( b_0 = 20 ): When house size is 0 (theoretically), base price is â‚¹20 lakhs.
* ( b_1 = 0.03 ): For every **additional square foot**, the house price increases by **â‚¹0.03 lakhs (â‚¹3,000)**.

---

### **Use Case:**

* The company can **predict future house prices** based on area.
* Helps in **pricing strategy**, **budget planning**, and **investment analysis**.

---

### ðŸ§  **Other Real-World Examples of SLR**

| Domain      | Example                            | Independent Variable (X) | Dependent Variable (Y) |
| ----------- | ---------------------------------- | ------------------------ | ---------------------- |
| Education   | Study hours vs Exam score          | Study hours              | Exam score             |
| Sales       | Advertising spend vs Sales revenue | Ad spend                 | Sales                  |
| Health      | Exercise time vs Weight loss       | Exercise time            | Weight loss            |
| Agriculture | Rainfall vs Crop yield             | Rainfall                 | Crop yield             |

---

Question 5: What is the method of least squares in linear regression?

**Answer:**

The **Method of Least Squares** is a mathematical technique used in **linear regression** to find the **best-fitting line** through a set of data points by **minimizing the sum of the squared errors (residuals)**.

---

### ðŸ“˜ **Concept**

In simple linear regression, we model the relationship as:
[
Y = b_0 + b_1X + \varepsilon
]

Here,

* ( Y ) = actual value
* ( \hat{Y} = b_0 + b_1X ) = predicted value
* ( \varepsilon = Y - \hat{Y} ) = error (residual)

The **goal** of the least squares method is to choose ( b_0 ) and ( b_1 ) such that the **sum of squared errors (SSE)** is minimized.

---

### âš™ï¸ **Mathematical Formulation**

We minimize the function:
[
S = \sum (Y_i - \hat{Y_i})^2 = \sum (Y_i - (b_0 + b_1X_i))^2
]

The values of ( b_0 ) and ( b_1 ) that minimize ( S ) are given by:

[
b_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}
]
[
b_0 = \bar{Y} - b_1\bar{X}
]

where:

* ( \bar{X} ) = mean of X values
* ( \bar{Y} ) = mean of Y values

---

### ðŸŽ¯ **Purpose**

The method ensures that the regression line is:

* As **close as possible** to all data points,
* With **minimum total squared distance** between actual and predicted values.

---

### ðŸ§  **Example**

Suppose we have data on **study hours (X)** and **scores (Y)**.
By applying the least squares method, we find:

[
\text{Score} = 40 + 5 \times (\text{Study Hours})
]

This line is the **best fit**, meaning it minimizes the overall squared difference between predicted and actual scores.

---

### **In Short**

| Term      | Meaning                                 |
| --------- | --------------------------------------- |
| Objective | Find best-fitting line                  |
| Method    | Minimize sum of squared errors          |
| Output    | Regression coefficients ( b_0, b_1 )    |
| Benefit   | Produces most accurate and unbiased fit |

---

Question 6: What is Logistic Regression? How does it differ from Linear Regression?

**Answer:**

### ðŸ”¹ **What is Logistic Regression?**

**Logistic Regression** is a **statistical method** used to model the relationship between a **dependent variable** (which is **categorical**) and one or more **independent variables**.

Unlike linear regression â€” which predicts **continuous** outcomes â€” logistic regression predicts the **probability** of a certain **class or event** (like â€œYes/Noâ€, â€œPass/Failâ€, â€œ0/1â€).

---

### ðŸ§® **Mathematical Form of Logistic Regression**

The model predicts the probability ( P ) that ( Y = 1 ):

[
P(Y=1|X) = \frac{1}{1 + e^{-(b_0 + b_1X)}}
]

where:

* ( P(Y=1|X) ): Probability that the outcome is 1
* ( e ): Base of natural logarithm (â‰ˆ 2.718)
* ( b_0 ): Intercept
* ( b_1 ): Coefficient for independent variable ( X )

Since probabilities must be between **0 and 1**, logistic regression uses the **sigmoid (S-shaped)** function to â€œsquashâ€ linear predictions into this range.

---

### âš–ï¸ **Purpose of Logistic Regression**

* To **classify** data into categories (e.g., spam vs non-spam, disease vs no disease).
* To **estimate probabilities** of belonging to a class.

---

### ðŸ”¸ **Difference Between Linear and Logistic Regression**

| Feature                | **Linear Regression**           | **Logistic Regression**                           |
| ---------------------- | ------------------------------- | ------------------------------------------------- |
| **Purpose**            | Predicts **continuous** values  | Predicts **categorical** outcomes (probabilities) |
| **Output Range**       | ( (-\infty, +\infty) )          | ( 0 ) to ( 1 )                                    |
| **Equation Type**      | ( Y = b_0 + b_1X )              | ( P = \frac{1}{1+e^{-(b_0+b_1X)}} )               |
| **Dependent Variable** | Continuous (e.g., sales, marks) | Binary or categorical (e.g., yes/no, 0/1)         |
| **Best Fit Line**      | Straight line                   | S-shaped curve (sigmoid)                          |
| **Error Measurement**  | Minimize sum of squared errors  | Uses maximum likelihood estimation (MLE)          |

---

### ðŸ§  **Example**

| Hours Studied (X) | Pass (Y) |
| ----------------- | -------- |
| 1                 | 0        |
| 3                 | 0        |
| 5                 | 1        |
| 7                 | 1        |
| 9                 | 1        |

Here, logistic regression can model the **probability of passing an exam** based on **study hours**, where output might be like:
[
P(\text{Pass}) = 0.85 \Rightarrow \text{Student likely passes.}
]

---

Question 7: Name and briefly describe three common evaluation metrics for regression models.

**Answer:**

When we build a **regression model**, we need to measure how well it predicts continuous values.
Here are **three common evaluation metrics** used for regression models ðŸ‘‡

---

### ðŸ”¹ **1. Mean Absolute Error (MAE)**

[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |Y_i - \hat{Y_i}|
]

**Meaning:**
MAE measures the **average absolute difference** between the actual values ((Y_i)) and the predicted values ((\hat{Y_i})).

**Interpretation:**

* Lower MAE = better model performance.
* It treats all errors equally (no squaring).

**Example:**
If MAE = 5, on average, predictions are off by **5 units**.

---

### ðŸ”¹ **2. Mean Squared Error (MSE)**

[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2
]

**Meaning:**
MSE calculates the **average of squared differences** between actual and predicted values.

**Interpretation:**

* Squaring penalizes **larger errors more heavily**.
* Lower MSE = more accurate model.

**Example:**
If MSE = 25, the **average squared error** is 25 (unitsÂ²).

---

### ðŸ”¹ **3. Root Mean Squared Error (RMSE)**

[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2}
]

**Meaning:**
RMSE is simply the **square root of MSE**, bringing the error value back to the **original units** of the target variable.

**Interpretation:**

* Easier to understand than MSE.
* Sensitive to large errors (since it squares differences).

**Example:**
If RMSE = 5, the modelâ€™s predictions are off by **about 5 units** on average.

---

### ðŸ§  **Summary Table**

| Metric | Formula                                       | Key Idea                  | Sensitive to Large Errors? | Unit                       |      |           |
| ------ | --------------------------------------------- | ------------------------- | -------------------------- | -------------------------- | ---- | --------- |
| MAE    | ( \frac{1}{n}\sum                             | Y_i - \hat{Y_i}           | )                          | Average of absolute errors | âŒ No | Same as Y |
| MSE    | ( \frac{1}{n}\sum(Y_i - \hat{Y_i})^2 )        | Average of squared errors | âœ… Yes                      | Squared                    |      |           |
| RMSE   | ( \sqrt{\frac{1}{n}\sum(Y_i - \hat{Y_i})^2} ) | Square root of MSE        | âœ… Yes                      | Same as Y                  |      |           |

---

Question 8: What is the purpose of the R-squared metric in regression analysis?

**Answer:**

### ðŸ”¹ **What is R-squared ( ( R^2 ) )?**

**R-squared** (also called the **Coefficient of Determination**) is a statistical metric used in **regression analysis** to measure how well the **independent variable(s)** explain the **variation** in the **dependent variable**.

It tells us **how well the regression line fits the data**.

---

### ðŸ§® **Formula**

[
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
]

Where:

* ( SS_{res} = \sum (Y_i - \hat{Y_i})^2 ) â†’ **Residual Sum of Squares** (unexplained variation)
* ( SS_{tot} = \sum (Y_i - \bar{Y})^2 ) â†’ **Total Sum of Squares** (total variation in Y)

---

### ðŸŽ¯ **Purpose**

* To measure the **goodness of fit** of a regression model.
* It shows **what percentage of the dependent variableâ€™s variation** is explained by the model.

---

### ðŸ“Š **Interpretation**

| RÂ² Value        | Interpretation                                    |
| --------------- | ------------------------------------------------- |
| 0               | Model explains **none** of the variation in Y     |
| 1               | Model explains **all** of the variation perfectly |
| Between 0 and 1 | Model explains **part** of the variation          |

For example:
If ( R^2 = 0.85 ), it means **85% of the variation in Y** is explained by X, and the remaining **15% is due to other factors or noise**.

---

### âš ï¸ **Important Notes**

* A **higher RÂ²** means a **better fit**, but not always a **better model** (it can be misleading if thereâ€™s overfitting).
* For models with multiple variables, an **Adjusted RÂ²** is often used, as it penalizes unnecessary predictors.

---

### ðŸ§  **Example**

If you are predicting **house prices** based on **size**,
and ( R^2 = 0.90 ):
â†’ 90% of the variation in house prices is explained by the house size,
â†’ 10% is due to other unknown factors.

---

Question 10: How do you interpret the coefficients in a simple linear regression model?

**Answer:**

In a **Simple Linear Regression** model, the equation is:

[
Y = b_0 + b_1X + \varepsilon
]

where:

* ( Y ): Dependent variable (predicted value)
* ( X ): Independent variable (input feature)
* ( b_0 ): Intercept
* ( b_1 ): Slope (coefficient)
* ( \varepsilon ): Error term

---

### ðŸ”¹ **Interpretation of Coefficients**

#### **1. Intercept (( b_0 ))**

* It represents the **predicted value of ( Y )** when ( X = 0 ).
* In other words, itâ€™s where the regression line **crosses the Y-axis**.
* It shows the **baseline value** of the dependent variable before considering X.

ðŸ“˜ **Example:**
If the regression equation is
[
\text{Sales} = 50 + 10 \times (\text{Advertising Spend})
]
then when Advertising Spend = 0, predicted Sales = 50 units.
â†’ 50 is the base level of sales even without advertising.

---

#### **2. Slope (( b_1 ))**

* It indicates **how much Y changes** for a **one-unit increase in X**.
* It shows the **direction** and **strength** of the relationship:

  * If ( b_1 > 0 ): As X increases, Y increases (positive relationship).
  * If ( b_1 < 0 ): As X increases, Y decreases (negative relationship).

ðŸ“˜ **Example:**
Using the same equation:
[
\text{Sales} = 50 + 10 \times (\text{Advertising Spend})
]
â†’ For every â‚¹1 increase in advertising spend, sales are expected to **increase by 10 units** on average.

---

### ðŸ§  **Summary Table**

| Coefficient     | Meaning   | Interpretation                         |
| --------------- | --------- | -------------------------------------- |
| ( b_0 )         | Intercept | Value of Y when X = 0                  |
| ( b_1 )         | Slope     | Change in Y for one-unit increase in X |
| Sign of ( b_1 ) | Direction | +ve â†’ Y increases, âˆ’ve â†’ Y decreases   |

---
"""

#Question 9: Write Python code to fit a simple linear regression model using scikit-learn
#and print the slope and intercept.
#(Include your Python code and output in the code box below.)

# ðŸ“˜ Import necessary libraries
from sklearn.linear_model import LinearRegression
import numpy as np

# ðŸ§® Sample data
# X = independent variable (must be 2D array)
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
# Y = dependent variable
Y = np.array([2, 4, 5, 4, 5])

# ðŸ”¹ Create a Linear Regression model
model = LinearRegression()

# ðŸ”¹ Fit the model to the data
model.fit(X, Y)

# ðŸ”¹ Print the slope (coefficient) and intercept
print("Slope (b1):", model.coef_[0])
print("Intercept (b0):", model.intercept_)

# Example of simple linear regression model:

# ðŸ“˜ Import libraries
import numpy as np
from sklearn.linear_model import LinearRegression

# ðŸ§® Step 1: Create sample data
# X = Advertising spend (in thousands)
# Y = Sales (in units)
X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)
Y = np.array([25, 45, 65, 80, 100])

# ðŸ”¹ Step 2: Create and fit the model
model = LinearRegression()
model.fit(X, Y)

# ðŸ”¹ Step 3: Get coefficients
slope = model.coef_[0]
intercept = model.intercept_

# ðŸ”¹ Step 4: Print the results
print("Intercept (b0):", intercept)
print("Slope (b1):", slope)

# ðŸ”¹ Step 5: Predict sales for 60k spend
predicted_sales = model.predict([[60]])
print("Predicted Sales for 60k spend:", predicted_sales[0])

"""# ***`THANK YOU ðŸ‘ `***



"""