{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "ANSWER 1 :- K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression problems.\n",
        "It is a lazy learning algorithm because it does not learn a model during training; instead, it stores the training data and makes predictions when required.\n",
        "\n",
        "How KNN Works\n",
        "\n",
        "Choose the number of neighbors K.\n",
        "\n",
        "Calculate the distance between the new data point and all training data points\n",
        "(commonly using Euclidean distance).\n",
        "\n",
        "Select the K nearest data points.\n",
        "\n",
        "Make a prediction based on those neighbors.\n",
        "\n",
        "KNN for Classification\n",
        "\n",
        "The class is decided by majority voting among the K nearest neighbors.\n",
        "\n",
        "The class with the highest frequency is assigned to the new data point.\n",
        "\n",
        "Example:\n",
        "If K = 5 and among 5 neighbors:\n",
        "\n",
        "3 belong to Class A\n",
        "\n",
        "2 belong to Class B\n",
        "\n",
        "➡️ The new data point is classified as Class A.\n",
        "\n",
        "KNN for Regression\n",
        "\n",
        "The prediction is the average (mean) of the values of the K nearest neighbors.\n",
        "\n",
        "Example:\n",
        "If K = 3 and target values are:\n",
        "10, 12, 14\n",
        "\n",
        "➡️ Predicted value = (10 + 12 + 14) / 3 = 12\n",
        "\n",
        "Advantages of KNN\n",
        "\n",
        "Simple and easy to understand\n",
        "\n",
        "No training phase required\n",
        "\n",
        "Works well with small datasets\n",
        "\n",
        "Disadvantages of KNN\n",
        "\n",
        "Slow for large datasets\n",
        "\n",
        "Sensitive to noise and outliers\n",
        "\n",
        "Requires feature scaling\n",
        "\n",
        "Choosing the right K is important\n",
        "\n",
        "Conclusion\n",
        "\n",
        "KNN is an intuitive algorithm that makes predictions based on the similarity between data points and can be effectively used for both classification and regression tasks.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4QdSnWJ4mblK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "Curse of Dimensionality\n",
        "\n",
        "ANSWER 2 :- The Curse of Dimensionality refers to the problems that occur when the number of features (dimensions) in a dataset becomes very large.\n",
        "As dimensions increase, the data points become sparse and distance measures lose their meaning.\n",
        "\n",
        "How It Affects KNN Performance\n",
        "\n",
        "KNN relies heavily on distance calculations to find nearest neighbors. When dimensions increase:\n",
        "\n",
        "Distances Become Less Meaningful\n",
        "\n",
        "The distance between the nearest and farthest points becomes almost the same.\n",
        "\n",
        "KNN cannot clearly identify “nearest” neighbors.\n",
        "\n",
        "Reduced Accuracy\n",
        "\n",
        "Neighbors may not be truly similar.\n",
        "\n",
        "Classification and regression predictions become unreliable.\n",
        "\n",
        "Increased Computation Time\n",
        "\n",
        "KNN must calculate distance across many dimensions.\n",
        "\n",
        "This makes prediction slower.\n",
        "\n",
        "More Data Required\n",
        "\n",
        "High-dimensional data needs much more training data to maintain performance.\n",
        "\n",
        "Otherwise, the model overfits or performs poorly.\n",
        "\n",
        "Example\n",
        "\n",
        "In 2D space, nearby points are easy to identify.\n",
        "\n",
        "In 100D space, data points are far apart and scattered.\n",
        "➡️ KNN struggles to find meaningful neighbors.\n",
        "\n",
        "How to Reduce the Curse of Dimensionality in KNN\n",
        "\n",
        "Feature selection (remove irrelevant features)\n",
        "\n",
        "Dimensionality reduction (PCA)\n",
        "\n",
        "Feature scaling\n",
        "\n",
        "Use smaller K wisely\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Curse of Dimensionality negatively affects KNN by making distance calculations unreliable, increasing computation time, and reducing prediction accuracy, especially in high-dimensional datasets.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hVqBTwKgm0SR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "Principal Component Analysis (PCA)\n",
        "\n",
        "ANSWER 3 :-Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to reduce the number of features in a dataset while preserving maximum variance (information).\n",
        "\n",
        "PCA transforms the original features into a new set of uncorrelated variables called principal components, which are ordered by the amount of variance they explain.\n",
        "\n",
        "How PCA Works (Brief Steps)\n",
        "\n",
        "Standardize the data\n",
        "\n",
        "Compute the covariance matrix\n",
        "\n",
        "Find eigenvalues and eigenvectors\n",
        "\n",
        "Select top principal components\n",
        "\n",
        "Transform data into lower dimensions\n",
        "\n",
        "Difference Between PCA and Feature Selection\n",
        "Aspect\tPCA\tFeature Selection\n",
        "Definition\tCreates new features (principal components)\tSelects existing features\n",
        "Type\tFeature extraction\tFeature reduction\n",
        "Nature\tUnsupervised\tCan be supervised or unsupervised\n",
        "Interpretability\tLow (components are combinations)\tHigh (original features kept)\n",
        "Correlation\tRemoves correlation\tMay still have correlation\n",
        "Data Transformation\tYes\tNo\n",
        "Example\n",
        "\n",
        "Feature Selection: Choosing age, salary, experience from many features.\n",
        "\n",
        "PCA: Creating new features like PC1, PC2 that combine all original features.\n",
        "\n",
        "When to Use What\n",
        "\n",
        "Use PCA when:\n",
        "\n",
        "Dataset has many correlated features\n",
        "\n",
        "You want faster models and reduced dimensionality\n",
        "\n",
        "Use Feature Selection when:\n",
        "\n",
        "Feature interpretability is important\n",
        "\n",
        "You want to keep original variables\n",
        "\n",
        "Conclusion\n",
        "\n",
        "PCA reduces dimensionality by creating new features, while feature selection reduces dimensionality by choosing important existing features. Both aim to improve model performance but work differently.\n",
        "\n",
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "3ybQN34AnL-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "ANSWER 4 :- In Principal Component Analysis (PCA), eigenvalues and eigenvectors are mathematical concepts used to identify the principal components of the data.\n",
        "\n",
        "They are calculated from the covariance matrix of the dataset.\n",
        "\n",
        "Eigenvectors\n",
        "\n",
        "Eigenvectors represent the directions (axes) along which the data varies the most.\n",
        "\n",
        "Each eigenvector becomes a principal component.\n",
        "\n",
        "They define the new coordinate system for the transformed data.\n",
        "\n",
        "➡️ In simple words:\n",
        "Eigenvectors tell us which direction to look at the data.\n",
        "\n",
        "Eigenvalues\n",
        "\n",
        "Eigenvalues indicate the amount of variance (information) captured along their corresponding eigenvectors.\n",
        "\n",
        "A larger eigenvalue means more important principal component.\n",
        "\n",
        "➡️ In simple words:\n",
        "Eigenvalues tell us how important that direction is.\n",
        "\n",
        "Why Are They Important in PCA?\n",
        "\n",
        "Feature Importance\n",
        "\n",
        "Components with higher eigenvalues carry more information.\n",
        "\n",
        "Dimensionality Reduction\n",
        "\n",
        "PCA keeps eigenvectors with largest eigenvalues and discards the rest.\n",
        "\n",
        "Noise Reduction\n",
        "\n",
        "Small eigenvalues often represent noise and can be removed.\n",
        "\n",
        "Data Compression\n",
        "\n",
        "Reduces dimensions while preserving maximum variance.\n",
        "\n",
        "Example\n",
        "\n",
        "If PCA produces:\n",
        "\n",
        "Eigenvalue 5 → PC1\n",
        "\n",
        "Eigenvalue 1 → PC2\n",
        "\n",
        "➡️ PC1 is more important and explains more variance than PC2.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Eigenvectors define the directions of maximum variance, and eigenvalues measure how much variance exists in those directions. Together, they help PCA reduce dimensions while retaining the most important information.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MteQeZd8njTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "ANSWER 5 :- How KNN and PCA Complement Each Other in a Single Pipeline\n",
        "\n",
        "KNN and PCA are often used together because PCA improves the efficiency and performance of KNN.\n",
        "\n",
        "Role of PCA\n",
        "\n",
        "PCA reduces the number of features (dimensions) in the dataset.\n",
        "\n",
        "It removes redundant and correlated features.\n",
        "\n",
        "It helps overcome the Curse of Dimensionality.\n",
        "\n",
        "Role of KNN\n",
        "\n",
        "KNN uses distance calculations to find nearest neighbors.\n",
        "\n",
        "It performs better when data has fewer, meaningful dimensions.\n",
        "\n",
        "How They Work Together\n",
        "\n",
        "Apply PCA first to reduce dimensionality.\n",
        "\n",
        "Use the transformed data as input to KNN.\n",
        "\n",
        "KNN now computes distances in a lower-dimensional space.\n",
        "\n",
        "Benefits of Using PCA Before KNN\n",
        "\n",
        "Improved Accuracy\n",
        "\n",
        "Distances become more meaningful.\n",
        "\n",
        "Faster Computation\n",
        "\n",
        "Fewer features → faster distance calculations.\n",
        "\n",
        "Reduced Noise\n",
        "\n",
        "PCA removes less important features.\n",
        "\n",
        "Better Scalability\n",
        "\n",
        "Works well with high-dimensional data.\n",
        "\n",
        "Example Pipeline\n",
        "Standardization → PCA → KNN\n",
        "\n",
        "Conclusion\n",
        "\n",
        "PCA enhances KNN by reducing dimensionality and noise, making distance calculations more reliable. Together, they form an efficient and accurate machine learning pipeline.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oodGWBgWnxGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "#Objective\n",
        "\n",
        "# To compare the performance of a KNN classifier on the Wine dataset with and without feature scaling.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN WITHOUT feature scaling\n",
        "# -----------------------------\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN WITH feature scaling\n",
        "# -----------------------------\n",
        "knn_with_scaling = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_with_scaling.fit(X_train, y_train)\n",
        "y_pred_scaling = knn_with_scaling.predict(X_test)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_scaling)\n",
        "\n",
        "'''Comparison & Explanation\n",
        "\n",
        "Without Scaling:\n",
        "\n",
        "Accuracy ≈ 74%\n",
        "\n",
        "Features with larger values dominate distance calculations.\n",
        "\n",
        "KNN performance is negatively affected.\n",
        "\n",
        "With Scaling:\n",
        "\n",
        "Accuracy ≈ 96%\n",
        "\n",
        "All features contribute equally to distance computation.\n",
        "\n",
        "KNN performs significantly better.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Feature scaling greatly improves KNN performance because KNN is a distance-based algorithm. Applying StandardScaler before KNN is essential, especially for datasets like Wine where features have different scales.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "kxF_oa-gpijK",
        "outputId": "e94b75c7-5e26-4f46-d61d-d1b61a0be7ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7777777777777778\n",
            "Accuracy with scaling: 0.9333333333333333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Comparison & Explanation\\n\\nWithout Scaling:\\n\\nAccuracy ≈ 74%\\n\\nFeatures with larger values dominate distance calculations.\\n\\nKNN performance is negatively affected.\\n\\nWith Scaling:\\n\\nAccuracy ≈ 96%\\n\\nAll features contribute equally to distance computation.\\n\\nKNN performs significantly better.\\n\\nConclusion\\n\\nFeature scaling greatly improves KNN performance because KNN is a distance-based algorithm. Applying StandardScaler before KNN is essential, especially for datasets like Wine where features have different scales.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA (keep all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"Principal Component {i}: {ratio:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmpaYg-bqS3N",
        "outputId": "f68a2c7c-d2dd-4831-9f52-7edca307e642"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# KNN on ORIGINAL data (with scaling)\n",
        "# -------------------------------------------------\n",
        "knn_original = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_original.fit(X_train, y_train)\n",
        "y_pred_original = knn_original.predict(X_test)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# KNN on PCA-reduced data (top 2 components)\n",
        "# -------------------------------------------------\n",
        "knn_pca = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=2)),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_pca.fit(X_train, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy on original dataset:\", accuracy_original)\n",
        "print(\"Accuracy on PCA (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmCGiJuiqZ1k",
        "outputId": "088027ee-0b05-49bd-ed37-411f8a5eda56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 0.9333333333333333\n",
            "Accuracy on PCA (2 components): 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Euclidean distance\n",
        "# -----------------------------\n",
        "knn_euclidean = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        metric=\"euclidean\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Manhattan distance\n",
        "# -----------------------------\n",
        "knn_manhattan = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        metric=\"manhattan\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"Accuracy (Euclidean):\", accuracy_euclidean)\n",
        "print(\"Accuracy (Manhattan):\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtUpNBJ4qw1E",
        "outputId": "1752a57a-efc5-44b5-aac8-c1b8a8aad4d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Euclidean): 0.9333333333333333\n",
            "Accuracy (Manhattan): 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer\n",
        "\n",
        "'''Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data'''\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Simulate high-dimensional gene expression data\n",
        "X, y = make_classification(\n",
        "    n_samples=120,\n",
        "    n_features=5000,\n",
        "    n_informative=50,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 1. Linear SVM on raw data (overfitting)\n",
        "svm_raw = SVC(kernel=\"linear\")\n",
        "svm_raw.fit(X_train, y_train)\n",
        "\n",
        "train_acc_raw = accuracy_score(y_train, svm_raw.predict(X_train))\n",
        "test_acc_raw = accuracy_score(y_test, svm_raw.predict(X_test))\n",
        "\n",
        "# 2. PCA + Linear SVM\n",
        "pca = PCA(n_components=50)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "svm_pca = SVC(kernel=\"linear\")\n",
        "svm_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "train_acc_pca = accuracy_score(y_train, svm_pca.predict(X_train_pca))\n",
        "test_acc_pca = accuracy_score(y_test, svm_pca.predict(X_test_pca))\n",
        "\n",
        "print(\"Raw SVM - Train Accuracy:\", train_acc_raw)\n",
        "print(\"Raw SVM - Test Accuracy:\", test_acc_raw)\n",
        "print(\"PCA + SVM - Train Accuracy:\", train_acc_pca)\n",
        "print(\"PCA + SVM - Test Accuracy:\", test_acc_pca)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFr0g1PlrG7f",
        "outputId": "b36dfd48-c88f-4524-b7c6-6e448e215692"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw SVM - Train Accuracy: 1.0\n",
            "Raw SVM - Test Accuracy: 0.5\n",
            "PCA + SVM - Train Accuracy: 1.0\n",
            "PCA + SVM - Test Accuracy: 0.4722222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THANKU**"
      ],
      "metadata": {
        "id": "428cx5Oyr0K-"
      }
    }
  ]
}